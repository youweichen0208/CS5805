{
  "hash": "2ea1ace80a6321e9906901fb23acfadb",
  "result": {
    "markdown": "---\ntitle: Clustering\nauthor: Youwei Chen\ndate: '2023-11-25'\ncategories:\n  - Machine Learning\n  - Python\n  - Clustering\n---\n\n### 1. Document Clustering Introduction:\n\nClustering is a fundamental technique in machine learning that involves grouping data with close and similar properties/patterns together. This blog specifically focuses on document clustering and makes an analysis of finding the sentences with closest and highest similarities. There are a couple of steps to achieve this in the later sections: 1. data clean and initialization 2. compute the cosine distance 3. find the closest cluster 4. merge 5. visualization\n\n### 2. Import the libraries:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport re\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\nfrom scipy.spatial.distance import cosine\n```\n:::\n\n\n### 3. Open the document file:\n\nBelow is the code to open the \"words.txt\" file and each line is an element in the **sentences** list.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nwith open('words.txt', 'r') as file:\n    sentences = file.readlines()\nsentences\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n['In comparison to dogs, cats have not undergone major changes during the domestication process.\\n',\n 'As cat simply catenates streams of bytes, it can be also used to concatenate binary files, where it will just concatenate sequence of bytes.\\n',\n 'A common interactive use of cat for a single file is to output the content of a file to standard output.\\n',\n 'Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals.\\n',\n 'In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin.\\n',\n 'The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes.\\n',\n 'Domestic cats are similar in size to the other members of the genus Felis, typically weighing between 4 and 5 kg (8.8 and 11.0 lb).\\n',\n 'However, if the output is piped or redirected, cat is unnecessary.\\n',\n 'cat with one named file is safer where human error is a concern - one wrong use of the default redirection symbol \">\" instead of \"<\" (often adjacent on keyboards) may permanently delete the file you were just needing to read.\\n',\n 'In terms of legibility, a sequence of commands starting with cat and connected by pipes has a clear left-to-right flow of information.\\n',\n 'Cat command is one of the basic commands that you learned when you started in the Unix / Linux world.\\n',\n 'Using cat command, the lines received from stdin can be redirected to a new file using redirection symbols.\\n',\n 'When you type simply cat command without any arguments, it just receives the stdin content and displays it in the stdout.\\n',\n 'Leopard was released on October 26, 2007 as the successor of Tiger (version 10.4), and is available in two editions.\\n',\n 'According to Apple, Leopard contains over 300 changes and enhancements over its predecessor, Mac OS X Tiger.\\n',\n 'As of Mid 2010, some Apple computers have firmware factory installed which will no longer allow installation of Mac OS X Leopard.\\n',\n 'Since Apple moved to using Intel processors in their computers, the OSx86 community has developed and now also allows Mac OS X Tiger and later releases to be installed on non-Apple x86-based computers.\\n',\n \"OS X Mountain Lion was released on July 25, 2012 for purchase and download through Apple's Mac App Store, as part of a switch to releasing OS X versions online and every year.\\n\",\n 'Apple has released a small patch for the three most recent versions of Safari running on OS X Yosemite, Mavericks, and Mountain Lion.\\n',\n 'The Mountain Lion release marks the second time Apple has offered an incremental upgrade, rather than releasing a new cat entirely.\\n',\n \"Mac OS X Mountain Lion installs in place, so you won't need to create a separate disk or run the installation off an external drive.\\n\",\n \"The fifth major update to Mac OS X, Leopard, contains such a mountain of features - more than 300 by Apple's count.\\n\"]\n```\n:::\n:::\n\n\n### 4. Tokenization and Data Clean:\n\nFor each sentence in the document, it first converts the sentence to lowercase, tokenize the sentence, and removes the empty words. The **tokenized_sentences** contains a list of cleaned words. Also, the **unique_words** contains all unique words found across the sentences in the document.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nunique_words = set()\ntokenized_sentences = []\n\nfor sentence in sentences:\n    sentence = sentence.lower()\n    words = re.split('[^a-z]', sentence)\n    words = [word for word in words if word]\n    tokenized_sentences.append(words)\n    unique_words.update(words)\nlen(tokenized_sentences)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n22\n```\n:::\n:::\n\n\n### 5. **Dataframe** setup:\n\nThe **dataframe** is structured to represent the frequency count of each unique word across sentences. Its columns correspond to the unique words, and each row corresponds to a sentence. The **dataframe** serves as the representation of the frequency distribution of words in the document.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nsentences_df = pd.DataFrame(index=range(len(tokenized_sentences)), columns=list(unique_words))\nsentences_df.fillna(0, inplace=True)\n\nfor idx, sentence in enumerate(tokenized_sentences):\n    for word in sentence:\n        sentences_df.loc[idx, word] += 1\nsentences_df\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>its</th>\n      <th>as</th>\n      <th>incremental</th>\n      <th>their</th>\n      <th>won</th>\n      <th>use</th>\n      <th>small</th>\n      <th>chromosomes</th>\n      <th>arguments</th>\n      <th>october</th>\n      <th>...</th>\n      <th>mice</th>\n      <th>will</th>\n      <th>dogs</th>\n      <th>adjacent</th>\n      <th>wild</th>\n      <th>major</th>\n      <th>releases</th>\n      <th>were</th>\n      <th>unix</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>22 rows Ã— 254 columns</p>\n</div>\n```\n:::\n:::\n\n\n### 6. Compute the cosine distance:\n\nCosine distance is often used as the measure of text similarity between sentences. The code below computes the cosine distance for each sentence in the Dataframe with respect to the sentence at index 2. The **cosine_distance** is the collection that represents the distance of the sentence at index 2 to the other sentences(include index 2).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnum_rows = sentences_df.shape[0]\ncosine_distance = [cosine(sentences_df.loc[2], sentences_df.loc[i]) for i in range(num_rows)]\ncosine_distance\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n[0.8644738145642124,\n 0.7908349933664811,\n 0,\n 0.9668503227934102,\n 0.7532824181024156,\n 0.9244071053981545,\n 0.7971629788651561,\n 0.7187156614369028,\n 0.5364474653494467,\n 0.5517892714996024,\n 0.7885277869449276,\n 0.6598319742916955,\n 0.8647753192434373,\n 0.8360159876618425,\n 0.9203180927110404,\n 0.8590185246299518,\n 0.8740118423302576,\n 0.7806366011571673,\n 0.7532824181024156,\n 0.7885277869449276,\n 0.8011019367604613,\n 0.704915554574673]\n```\n:::\n:::\n\n\n### 7. Sort and find the two sentences with the closest distance:\n\nIn the previous section, we computed the cosine distance and got a list of distances to the sentence at index 2. Now this section is to sort the list and get 2 closest sentences to the sentence at index 2.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nindexed_distances = list(enumerate(cosine_distance))\nsorted_distances = sorted(indexed_distances, key=lambda x: x[1])\nclosest_sentences = sorted_distances[1:3]\nclosest_sentences\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[(8, 0.5364474653494467), (9, 0.5517892714996024)]\n```\n:::\n:::\n\n\nAs we can see from the output, the two closest sentences to the sentence at index 2 are the sentence at index 8 and sentence at index 9. In the next section we will use dendrogram to verify and visualize the cosine distances.\n\n### 8. Visualization using dendrogram:\n\nA dendrogram is a very useful visualization diagram that displays the sequence of merges in hierarchical clustering. By looking at the dendrogram, we can get a sense of how closely related clusters are.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nvisual = linkage(sentences_df, method='single', metric=\"cosine\")\nfig = plt.figure(figsize=(5, 5))\ndn = dendrogram(visual)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=424 height=419}\n:::\n:::\n\n\nAs we can see in the dendrogram, the sentence at index 8 and the sentence at index 9 are exactly the closest distance to the sentence at index 2, which matches with the answers in section 7.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}