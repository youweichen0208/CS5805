[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/probability-theory/index.html",
    "href": "posts/probability-theory/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "1. Use Case: Email spam detection\nThe goal of this blog is to develop a detection system and data visualization for email spams.Each email‚Äôs text in the dataset can be considered as a random variable. We will use The Naive Bayers model and probablity theory to analyze the probability that an email is spam given the text in that email. The process involves training the Naive Bayes model using a provided dataset, enabling the model to learn and make predictions on new, unseen data.\n\n\n2. Import the libraries:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n\n\n3. Display the dataset\nBelow is the code to load ‚Äúemails.csv‚Äù file into the pandas dataframe.\n\nemail_df = pd.read_csv(\"emails.csv\")\nemail_df['category'] = email_df['spam'].replace({1: 'spam', 0: 'not spam'})\nemail_df\n\n\n\n\n\n\n\n\ntext\nspam\ncategory\n\n\n\n\n0\nSubject: naturally irresistible your corporate...\n1\nspam\n\n\n1\nSubject: the stock trading gunslinger fanny i...\n1\nspam\n\n\n2\nSubject: unbelievable new homes made easy im ...\n1\nspam\n\n\n3\nSubject: 4 color printing special request add...\n1\nspam\n\n\n4\nSubject: do not have money , get software cds ...\n1\nspam\n\n\n...\n...\n...\n...\n\n\n5723\nSubject: re : research and development charges...\n0\nnot spam\n\n\n5724\nSubject: re : receipts from visit jim , than...\n0\nnot spam\n\n\n5725\nSubject: re : enron case study update wow ! a...\n0\nnot spam\n\n\n5726\nSubject: re : interest david , please , call...\n0\nnot spam\n\n\n5727\nSubject: news : aurora 5 . 2 update aurora ve...\n0\nnot spam\n\n\n\n\n5728 rows √ó 3 columns\n\n\n\n\n\n4. groupby based on category column (‚Äúspam‚Äù‚Äù and ‚Äúnot spam‚Äù)\ngroupby will group the spam and not spam in the category column, and it will provide us with detailed analysis of the groups spam and not spam.\n\nemail_df.groupby('category').describe()\n\n\n\n\n\n\n\n\nspam\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ncategory\n\n\n\n\n\n\n\n\n\n\n\n\nnot spam\n4360.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nspam\n1368.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n5. use scikit-learn to split data into training set:\nTo evaluate the model, train_test_split is used to split spam email dataset into training and testing sets. In this case, 20% of the data will be used for tesing, and 80% of the data will be used for training. Additionally, CountVectorizer is applied to convert the text data into a matrix of words counts, and fit_transform is used to fit the vectorizer to the training data and build the model.\n\nx_train, x_test, y_train, y_test = train_test_split(email_df.text, email_df.spam, test_size=0.20)\ncv = CountVectorizer()\nx_train_count = cv.fit_transform(x_train.values)\nx_train_count.toarray()\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])\n\n\n\n\n6. create a Multinomial Bayes model:\nIt will train the model, which enables it to make predictions on new data.\n\n#train model\nmodel = MultinomialNB()\nmodel.fit(x_train_count, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\n\n7. Below is the examples of spam and non-spam emails:\n\nexample 1:\n\nemail_ham = [\"Please come prepared with any updates or materials you'd like to share with the team.\"]\nemail_ham_count = cv.transform(email_ham)\nmodel.predict(email_ham_count)\n\narray([0])\n\n\nThe message in the example 1 is not a spam email, and it appears to be the team communication in a business context. The training model also gives a value of 0 which means it predicts that it is not a spam email.\n\n\nexample 2:\n\n# pre-test spam\nemail_spam = [\"ou are the lucky winner of our exclusive giveaway! ü•≥ You've won a brand new iPhone 12 and a $500 shopping voucher!\"]\nemail_spam_count = cv.transform(email_spam)\nmodel.predict(email_spam_count)\n\narray([1])\n\n\nThe message in the example 2 is a spam email, and it appears to be one of the tricks to promote their products. The training model also gives a value of 1 which means it predicts that it is a spam email.\n\n\n\n8. Prediction accuracy:\nBelow shows the accuracy of the model on the test data.\n\nx_test_count = cv.transform(x_test)\nmodel.score(x_test_count, y_test)\n\n0.9930191972076788\n\n\n\n\n9. ROC curve to evaluate the performance of our email spam detection system:\nThe ROC curve is used to understand and visualize the performance of our email spam detection system. To interpret ROC curve, the closer the ROC curve to the true postive rate axis, the better the model‚Äôs performance.\n\ny_prob = model.predict_proba(x_test_count)[:, 1]\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "posts/anomaly-outlier-detection/index.html",
    "href": "posts/anomaly-outlier-detection/index.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "1. Introduction:\nThis blog will utilize both rule-based VADER sentiment analysis tool and machine learning pre-trained RoBERTa model to perform analysis of sentiment in Amazon reviews. It will apply sentiment analysis models to classify reviews into categories such as ‚Äúpositive‚Äù, ‚Äúneutral‚Äù, and ‚Äúnegative‚Äù. The visualizations at end will also show the relationship between different sentiment classifiers.\n\n\n2. Import the libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom tqdm.notebook import tqdm \nfrom scipy.special import softmax\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n\n3. Load the amazon_reviews csv file:\n\namazon_df = pd.read_csv(\"amazon_reviews.csv\")\n\n\n\n4. Bar plot to show the number of reviews in different ratings:\n\nvis = amazon_df['rating'].value_counts().sort_index().plot(kind='bar', title='number of reviews vs. ratings', figsize=(10,5))\nvis.set_xlabel(\"Review ratings\")\nvis.set_ylabel(\"number of reviews\")\nvis.set_xticklabels(vis.get_xticklabels(), rotation=1)\nplt.show()\n\n\n\n\n\n\n5. Apply SentimentIntensityAnalyzer:\nThe code below uses the VADER sentiment analysis tool from NLTK to give scores for each Amazon reviews. Based on the scores, the new column sentiment will classify ‚Äònegative‚Äô, ‚Äòpositive‚Äô, ‚Äòneutral‚Äô based on the scores. The dataframe vader_nltk stores polarity scores and sentiment labels.\n\ndef get_sentiment(score):\n    if score &lt; -0.05:\n        return 'negative'\n    elif score &gt; 0.05:\n        return 'positive'\n    else:\n        return 'neutral'\n\nsia = SentimentIntensityAnalyzer()\nres = {}\nfor i, row in tqdm(amazon_df.iterrows(), total=len(amazon_df)):\n    text = str(row['reviewText'])\n    myid = row['id']\n    polarity_scores = sia.polarity_scores(text)\n    res[myid] = {**polarity_scores, 'sentiment': get_sentiment(polarity_scores['compound'])}\n\nvader_nltk = pd.DataFrame.from_dict(res, orient='index').reset_index()\nvader_nltk.columns = ['id'] + list(vader_nltk.columns[1:])\nvader_nltk = pd.merge(vader_nltk, amazon_df, on='id', how='left')\nvader_nltk.head()\n\n\n\n\n\n\n\n\n\n\n\nid\nneg\nneu\npos\ncompound\nsentiment\nreviewerName\nrating\nreviewText\nreviewTime\nday_diff\nhelpful_yes\nhelpful_no\ntotal_vote\nscore_pos_neg_diff\nscore_average_rating\nwilson_lower_bound\n\n\n\n\n0\n0\n0.688\n0.312\n0.000\n-0.2960\nnegative\nNaN\n4.0\nNo issues.\n2014-07-23\n138\n0\n0\n0\n0\n0.0\n0.0\n\n\n1\n2\n0.077\n0.923\n0.000\n-0.3089\nnegative\n1K3\n4.0\nit works as expected. I should have sprung for...\n2012-12-23\n715\n0\n0\n0\n0\n0.0\n0.0\n\n\n2\n11\n0.000\n1.000\n0.000\n0.0000\nneutral\n98020\n3.0\nIt works, but file writes are a bit slower tha...\n2013-11-20\n383\n0\n0\n0\n0\n0.0\n0.0\n\n\n3\n32\n0.088\n0.774\n0.138\n0.5255\npositive\nAbdulrahman J. Alrashed \"dr34m3r\"\n1.0\nI bought 2 of those SanDisk 32 GB microSD , us...\n2013-03-08\n640\n0\n0\n0\n0\n0.0\n0.0\n\n\n4\n33\n0.000\n0.632\n0.368\n0.9153\npositive\nAbel Feliciano \"Ace Master\"\n4.0\nThe memory card is an excellent condition and ...\n2012-11-09\n759\n0\n0\n0\n0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n6. the boxplot to show the distribution of ‚Äòneg‚Äô, ‚Äòneu‚Äô, ‚Äòpos‚Äô for each rating.\n\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\nsns.boxplot(data=vader_nltk, x='rating', y='neg', ax=axs[0])\nsns.boxplot(data=vader_nltk, x='rating', y='neu', ax=axs[1])\nsns.boxplot(data=vader_nltk, x='rating', y='pos', ax=axs[2])\naxs[0].set_title('Negative sentiment')\naxs[1].set_title('Neutral sentiment')\naxs[2].set_title('Positive sentiment')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n7. Hugging Face Transformers to load a pre-trained model:\nIt sets up a sentiment analysis model based on RoBERTa architecture for analyzing the Amazon reviews.\n\nMODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\n\n\n8. VADER Sentiment analysis vs.¬†RoBERTa Model Sentiment Analysis:\n\n(1). VADER score about the review at index 30:\n\nexample_review = amazon_df['reviewText'][30]\nexample_review\nprint(\"Vader sentiment analysis scores for the review at index 30 is \", sia.polarity_scores(example_review))\n\nVader sentiment analysis scores for the review at index 30 is  {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6249}\n\n\n\n\n(2).RoBERTa score about the review at index 30:\n\nencoded_review = tokenizer(example_review, return_tensors='pt')\noutput = model(**encoded_review)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\nscores_dict = {\n  'roberta_neg': scores[0],\n  'roberta_neu': scores[1],\n  'roberta_pos': scores[2]\n}\n\nprint(\"Roberta scores for the review at index 30 is \", scores_dict)\n\nRoberta scores for the review at index 30 is  {'roberta_neg': 0.25840575, 'roberta_neu': 0.34866926, 'roberta_pos': 0.39292496}\n\n\n\n\n\n9. RoBERTa score and VADER score Integration:\nThe res dictionary combines and contains both the VADER sentiment analysis result and the pre-trained model RoBERTa model‚Äôs result.\n\ndef roberta_scores(example_review):\n  encoded_review = tokenizer(example_review, return_tensors='pt')\n  output = model(**encoded_review)\n  scores = output[0][0].detach().numpy()\n  scores = softmax(scores)\n  scores_dict = {\n    'roberta_neg': scores[0],\n    'roberta_neu': scores[1],\n    'roberta_pos': scores[2]\n  }\n  return scores_dict\nres = {}\nfor i, row in tqdm(amazon_df.iterrows(), total=len(amazon_df)):\n    try:\n        review = row['reviewText']\n        review_id = row['id']\n        if isinstance(review, str):\n            vader_result = sia.polarity_scores(review)\n            vader_result2 = {f\"vader_{key}\": value for key, value in vader_result.items()}\n            roberta_result = roberta_scores(review)\n            res[review_id] = {**vader_result2, **roberta_result}\n        else:\n            print(f'Skipped id: {review_id} due to non-string review')\n    except RuntimeError:\n        continue\ndict_list = list(res.items())[:5]\nfor item in dict_list:\n    print(item)\n\n\n\n\n(0, {'vader_neg': 0.688, 'vader_neu': 0.312, 'vader_pos': 0.0, 'vader_compound': -0.296, 'roberta_neg': 0.07495943, 'roberta_neu': 0.5892151, 'roberta_pos': 0.33582547})\n(2, {'vader_neg': 0.077, 'vader_neu': 0.923, 'vader_pos': 0.0, 'vader_compound': -0.3089, 'roberta_neg': 0.42375654, 'roberta_neu': 0.4483106, 'roberta_pos': 0.12793283})\n(11, {'vader_neg': 0.0, 'vader_neu': 1.0, 'vader_pos': 0.0, 'vader_compound': 0.0, 'roberta_neg': 0.2745669, 'roberta_neu': 0.4989962, 'roberta_pos': 0.22643691})\n(32, {'vader_neg': 0.088, 'vader_neu': 0.774, 'vader_pos': 0.138, 'vader_compound': 0.5255, 'roberta_neg': 0.54347664, 'roberta_neu': 0.3384522, 'roberta_pos': 0.118071124})\n(33, {'vader_neg': 0.0, 'vader_neu': 0.632, 'vader_pos': 0.368, 'vader_compound': 0.9153, 'roberta_neg': 0.0029975341, 'roberta_neu': 0.026594352, 'roberta_pos': 0.97040814})\n\n\n\n\n10. Merge into dataframe:\n\nmodel_df = pd.DataFrame(res).T\nmodel_df = model_df.reset_index().rename(columns={'index':'id'})\nmodel_df = model_df.merge(amazon_df, how='left')\nmodel_df.head()\n\n\n\n\n\n\n\n\nid\nvader_neg\nvader_neu\nvader_pos\nvader_compound\nroberta_neg\nroberta_neu\nroberta_pos\nreviewerName\nrating\nreviewText\nreviewTime\nday_diff\nhelpful_yes\nhelpful_no\ntotal_vote\nscore_pos_neg_diff\nscore_average_rating\nwilson_lower_bound\n\n\n\n\n0\n0\n0.688\n0.312\n0.000\n-0.2960\n0.074959\n0.589215\n0.335825\nNaN\n4.0\nNo issues.\n2014-07-23\n138\n0\n0\n0\n0\n0.0\n0.0\n\n\n1\n2\n0.077\n0.923\n0.000\n-0.3089\n0.423757\n0.448311\n0.127933\n1K3\n4.0\nit works as expected. I should have sprung for...\n2012-12-23\n715\n0\n0\n0\n0\n0.0\n0.0\n\n\n2\n11\n0.000\n1.000\n0.000\n0.0000\n0.274567\n0.498996\n0.226437\n98020\n3.0\nIt works, but file writes are a bit slower tha...\n2013-11-20\n383\n0\n0\n0\n0\n0.0\n0.0\n\n\n3\n32\n0.088\n0.774\n0.138\n0.5255\n0.543477\n0.338452\n0.118071\nAbdulrahman J. Alrashed \"dr34m3r\"\n1.0\nI bought 2 of those SanDisk 32 GB microSD , us...\n2013-03-08\n640\n0\n0\n0\n0\n0.0\n0.0\n\n\n4\n33\n0.000\n0.632\n0.368\n0.9153\n0.002998\n0.026594\n0.970408\nAbel Feliciano \"Ace Master\"\n4.0\nThe memory card is an excellent condition and ...\n2012-11-09\n759\n0\n0\n0\n0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n11. Heatmap to show the relationship between VADER Sentiment analysis tool and RoBERTa training model:\nThe heatmap is applied here to explore the correlation between the VADER model classifications and the RoBERTa model classifications.\n\nplt.figure(figsize=(4, 3))\ncorrelation = model_df[['vader_neg', 'vader_neu', 'vader_pos', 'roberta_neg', 'roberta_neu', 'roberta_pos']].corr()\nsns.heatmap(correlation, annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom the heatmap shown above, the correlation coefficient of two models‚Äô relationship:\n\nvader_neg - roberta_neg: 0.35\nvader_neu - roberta_neu: 0.36\nvader_pos - roberta_pos: 0.5"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "1. Document Clustering Introduction:\nClustering is a fundamental technique in machine learning that involves grouping data with close and similar properties/patterns together. This blog specifically focuses on document clustering and makes an analysis of finding the sentences with closest and highest similarities. There are a couple of steps to achieve this in the later sections: 1. data clean and initialization 2. compute the cosine distance 3. find the closest cluster 4. merge 5. visualization\n\n\n2. Import the libraries:\n\nimport re\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\nfrom scipy.spatial.distance import cosine\n\n\n\n3. Open the document file:\nBelow is the code to open the ‚Äúwords.txt‚Äù file and each line is an element in the sentences list.\n\nwith open('words.txt', 'r') as file:\n    sentences = file.readlines()\nsentences\n\n['In comparison to dogs, cats have not undergone major changes during the domestication process.\\n',\n 'As cat simply catenates streams of bytes, it can be also used to concatenate binary files, where it will just concatenate sequence of bytes.\\n',\n 'A common interactive use of cat for a single file is to output the content of a file to standard output.\\n',\n 'Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals.\\n',\n 'In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin.\\n',\n 'The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes.\\n',\n 'Domestic cats are similar in size to the other members of the genus Felis, typically weighing between 4 and 5 kg (8.8 and 11.0 lb).\\n',\n 'However, if the output is piped or redirected, cat is unnecessary.\\n',\n 'cat with one named file is safer where human error is a concern - one wrong use of the default redirection symbol \"&gt;\" instead of \"&lt;\" (often adjacent on keyboards) may permanently delete the file you were just needing to read.\\n',\n 'In terms of legibility, a sequence of commands starting with cat and connected by pipes has a clear left-to-right flow of information.\\n',\n 'Cat command is one of the basic commands that you learned when you started in the Unix / Linux world.\\n',\n 'Using cat command, the lines received from stdin can be redirected to a new file using redirection symbols.\\n',\n 'When you type simply cat command without any arguments, it just receives the stdin content and displays it in the stdout.\\n',\n 'Leopard was released on October 26, 2007 as the successor of Tiger (version 10.4), and is available in two editions.\\n',\n 'According to Apple, Leopard contains over 300 changes and enhancements over its predecessor, Mac OS X Tiger.\\n',\n 'As of Mid 2010, some Apple computers have firmware factory installed which will no longer allow installation of Mac OS X Leopard.\\n',\n 'Since Apple moved to using Intel processors in their computers, the OSx86 community has developed and now also allows Mac OS X Tiger and later releases to be installed on non-Apple x86-based computers.\\n',\n \"OS X Mountain Lion was released on July 25, 2012 for purchase and download through Apple's Mac App Store, as part of a switch to releasing OS X versions online and every year.\\n\",\n 'Apple has released a small patch for the three most recent versions of Safari running on OS X Yosemite, Mavericks, and Mountain Lion.\\n',\n 'The Mountain Lion release marks the second time Apple has offered an incremental upgrade, rather than releasing a new cat entirely.\\n',\n \"Mac OS X Mountain Lion installs in place, so you won't need to create a separate disk or run the installation off an external drive.\\n\",\n \"The fifth major update to Mac OS X, Leopard, contains such a mountain of features - more than 300 by Apple's count.\\n\"]\n\n\n\n\n4. Tokenization and Data Clean:\nFor each sentence in the document, it first converts the sentence to lowercase, tokenize the sentence, and removes the empty words. The tokenized_sentences contains a list of cleaned words. Also, the unique_words contains all unique words found across the sentences in the document.\n\nunique_words = set()\ntokenized_sentences = []\n\nfor sentence in sentences:\n    sentence = sentence.lower()\n    words = re.split('[^a-z]', sentence)\n    words = [word for word in words if word]\n    tokenized_sentences.append(words)\n    unique_words.update(words)\nlen(tokenized_sentences)\n\n22\n\n\n\n\n5. Dataframe setup:\nThe dataframe is structured to represent the frequency count of each unique word across sentences. Its columns correspond to the unique words, and each row corresponds to a sentence. The dataframe serves as the representation of the frequency distribution of words in the document.\n\nsentences_df = pd.DataFrame(index=range(len(tokenized_sentences)), columns=list(unique_words))\nsentences_df.fillna(0, inplace=True)\n\nfor idx, sentence in enumerate(tokenized_sentences):\n    for word in sentence:\n        sentences_df.loc[idx, word] += 1\nsentences_df\n\n\n\n\n\n\n\n\nits\nas\nincremental\ntheir\nwon\nuse\nsmall\nchromosomes\narguments\noctober\n...\nmice\nwill\ndogs\nadjacent\nwild\nmajor\nreleases\nwere\nunix\noutput\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n3\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n8\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n15\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n17\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n19\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n20\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n22 rows √ó 254 columns\n\n\n\n\n\n6. Compute the cosine distance:\nCosine distance is often used as the measure of text similarity between sentences. The code below computes the cosine distance for each sentence in the Dataframe with respect to the sentence at index 2. The cosine_distance is the collection that represents the distance of the sentence at index 2 to the other sentences(include index 2).\n\nnum_rows = sentences_df.shape[0]\ncosine_distance = [cosine(sentences_df.loc[2], sentences_df.loc[i]) for i in range(num_rows)]\ncosine_distance\n\n[0.8644738145642124,\n 0.7908349933664811,\n 0,\n 0.9668503227934102,\n 0.7532824181024156,\n 0.9244071053981545,\n 0.7971629788651561,\n 0.7187156614369028,\n 0.5364474653494467,\n 0.5517892714996024,\n 0.7885277869449276,\n 0.6598319742916955,\n 0.8647753192434373,\n 0.8360159876618425,\n 0.9203180927110404,\n 0.8590185246299518,\n 0.8740118423302576,\n 0.7806366011571673,\n 0.7532824181024156,\n 0.7885277869449276,\n 0.8011019367604613,\n 0.704915554574673]\n\n\n\n\n7. Sort and find the two sentences with the closest distance:\nIn the previous section, we computed the cosine distance and got a list of distances to the sentence at index 2. Now this section is to sort the list and get 2 closest sentences to the sentence at index 2.\n\nindexed_distances = list(enumerate(cosine_distance))\nsorted_distances = sorted(indexed_distances, key=lambda x: x[1])\nclosest_sentences = sorted_distances[1:3]\nclosest_sentences\n\n[(8, 0.5364474653494467), (9, 0.5517892714996024)]\n\n\nAs we can see from the output, the two closest sentences to the sentence at index 2 are the sentence at index 8 and sentence at index 9. In the next section we will use dendrogram to verify and visualize the cosine distances.\n\n\n8. Visualization using dendrogram:\nA dendrogram is a very useful visualization diagram that displays the sequence of merges in hierarchical clustering. By looking at the dendrogram, we can get a sense of how closely related clusters are.\n\nvisual = linkage(sentences_df, method='single', metric=\"cosine\")\nfig = plt.figure(figsize=(5, 5))\ndn = dendrogram(visual)\nplt.show()\n\n\n\n\nAs we can see in the dendrogram, the sentence at index 8 and the sentence at index 9 are exactly the closest distance to the sentence at index 2, which matches with the answers in section 7."
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nNLP\n\n\ncode\n\n\nsentiment analysis\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nmachine learning\n\n\npython\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\nNo matching items"
  }
]