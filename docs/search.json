[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/probability-theory/index.html",
    "href": "posts/probability-theory/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "1. Use Case: Email spam detection\nThe goal of this blog is to develop a detection system and data visualization for email spams.Each email‚Äôs text in the dataset can be considered as a random variable. We will use The Naive Bayers model and probablity theory to analyze the probability that an email is spam given the text in that email. The process involves training the Naive Bayes model using a provided dataset, enabling the model to learn and make predictions on new, unseen data.\n\n\n2. Import the libraries:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n\n\n3. Display the dataset\nBelow is the code to load ‚Äúemails.csv‚Äù file into the pandas dataframe.\n\nemail_df = pd.read_csv(\"emails.csv\")\nemail_df['category'] = email_df['spam'].replace({1: 'spam', 0: 'not spam'})\nemail_df\n\n\n\n\n\n\n\n\ntext\nspam\ncategory\n\n\n\n\n0\nSubject: naturally irresistible your corporate...\n1\nspam\n\n\n1\nSubject: the stock trading gunslinger fanny i...\n1\nspam\n\n\n2\nSubject: unbelievable new homes made easy im ...\n1\nspam\n\n\n3\nSubject: 4 color printing special request add...\n1\nspam\n\n\n4\nSubject: do not have money , get software cds ...\n1\nspam\n\n\n...\n...\n...\n...\n\n\n5723\nSubject: re : research and development charges...\n0\nnot spam\n\n\n5724\nSubject: re : receipts from visit jim , than...\n0\nnot spam\n\n\n5725\nSubject: re : enron case study update wow ! a...\n0\nnot spam\n\n\n5726\nSubject: re : interest david , please , call...\n0\nnot spam\n\n\n5727\nSubject: news : aurora 5 . 2 update aurora ve...\n0\nnot spam\n\n\n\n\n5728 rows √ó 3 columns\n\n\n\n\n\n4. groupby based on category column (‚Äúspam‚Äù‚Äù and ‚Äúnot spam‚Äù)\ngroupby will group the spam and not spam in the category column, and it will provide us with detailed analysis of the groups spam and not spam.\n\nemail_df.groupby('category').describe()\n\n\n\n\n\n\n\n\nspam\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ncategory\n\n\n\n\n\n\n\n\n\n\n\n\nnot spam\n4360.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nspam\n1368.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n5. use scikit-learn to split data into training set:\nTo evaluate the model, train_test_split is used to split spam email dataset into training and testing sets. In this case, 20% of the data will be used for tesing, and 80% of the data will be used for training. Additionally, CountVectorizer is applied to convert the text data into a matrix of words counts, and fit_transform is used to fit the vectorizer to the training data and build the model.\n\nx_train, x_test, y_train, y_test = train_test_split(email_df.text, email_df.spam, test_size=0.20)\ncv = CountVectorizer()\nx_train_count = cv.fit_transform(x_train.values)\nx_train_count.toarray()\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])\n\n\n\n\n6. create a Multinomial Bayes model:\nIt will train the model, which enables it to make predictions on new data.\n\n#train model\nmodel = MultinomialNB()\nmodel.fit(x_train_count, y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\n\n7. Below is the examples of spam and non-spam emails:\n\nexample 1:\n\nemail_ham = [\"Please come prepared with any updates or materials you'd like to share with the team.\"]\nemail_ham_count = cv.transform(email_ham)\nmodel.predict(email_ham_count)\n\narray([0])\n\n\nThe message in the example 1 is not a spam email, and it appears to be the team communication in a business context. The training model also gives a value of 0 which means it predicts that it is not a spam email.\n\n\nexample 2:\n\n# pre-test spam\nemail_spam = [\"ou are the lucky winner of our exclusive giveaway! ü•≥ You've won a brand new iPhone 12 and a $500 shopping voucher!\"]\nemail_spam_count = cv.transform(email_spam)\nmodel.predict(email_spam_count)\n\narray([1])\n\n\nThe message in the example 2 is a spam email, and it appears to be one of the tricks to promote their products. The training model also gives a value of 1 which means it predicts that it is a spam email.\n\n\n\n8. Prediction accuracy:\nBelow shows the accuracy of the model on the test data.\n\nx_test_count = cv.transform(x_test)\nmodel.score(x_test_count, y_test)\n\n0.9921465968586387\n\n\n\n\n9. ROC curve to evaluate the performance of our email spam detection system:\nThe ROC curve is used to understand and visualize the performance of our email spam detection system. To interpret ROC curve, the closer the ROC curve to the true postive rate axis, the better the model‚Äôs performance.\n\ny_prob = model.predict_proba(x_test_count)[:, 1]\n\n# Compute the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "posts/anomaly-outlier-detection/index.html",
    "href": "posts/anomaly-outlier-detection/index.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "1.Introduction:\nThis blog aims to provide a comprehensive approach to identify anomalies in student math scores and visualizing the results. It uses boxplot to display the distribution of students‚Äô math scores and the Isolation Forest model to detect these anomalies.DBSCAN labels for scatterplot is used to better view the clusters and visualization.\n\n\n2. Import libraries:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n3. Read the CSV file:\n\nstudent_df = pd.read_csv('student_data.csv')\nstudent_df.head()\n\n\n\n\n\n\n\n\nstudent id\nmath_score\n\n\n\n\n0\n1\n95\n\n\n1\n2\n98\n\n\n2\n3\n92\n\n\n3\n4\n10000\n\n\n4\n5\n91\n\n\n\n\n\n\n\n\n\n4. Define the anomaly in the context:\nThe math score is considered as anomaly if the score is higher than 100. It creates a new column actualy_anomaly in the dataframe to indicate whether the data is anomaly.\n\nstudent_df['actual_anomaly'] = student_df['math_score'] &gt; 100\nstudent_df.head()\n\n\n\n\n\n\n\n\nstudent id\nmath_score\nactual_anomaly\n\n\n\n\n0\n1\n95\nFalse\n\n\n1\n2\n98\nFalse\n\n\n2\n3\n92\nFalse\n\n\n3\n4\n10000\nTrue\n\n\n4\n5\n91\nFalse\n\n\n\n\n\n\n\n\n\n5. Visualizing students‚Äô math scores using Boxplot:\nBoxplot is a great way of displaying and showing the distribution of students‚Äô math scores. It can show us about outliers intuitively and what their values are.\n\nsns.boxplot(x=student_df.math_score)\nplt.show()\n\n\n\n\n\n\n6. Fitting an Isolation Forest Model for Anomaly Detection:\nIt uses Isolation Forest model algorithm for anomaly detection. The model is fitted to the math_score column of the student_df DataFrame to detect any anomalies in the math scores of the students.\n\nmodel = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.15), max_features=1.0)\nmodel.fit(student_df[['math_score']])\n\nIsolationForest(contamination=0.15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.15)\n\n\n\n\n7. Predicting anomalies in Student Math Scores:\nIt uses trained Isolation Forest model to predict anomalies in the math_score column of the student_df dataframe. model.decision_function will compute and return the anomaly score for each math score.\n\nstudent_df['anomalies_scores'] = model.decision_function(student_df[['math_score']])\nstudent_df['anomaly'] = model.predict(student_df[['math_score']])\nstudent_df.head(20)\n\n\n\n\n\n\n\n\nstudent id\nmath_score\nactual_anomaly\nanomalies_scores\nanomaly\n\n\n\n\n0\n1\n95\nFalse\n0.028871\n1\n\n\n1\n2\n98\nFalse\n0.013930\n1\n\n\n2\n3\n92\nFalse\n0.046547\n1\n\n\n3\n4\n10000\nTrue\n-0.377246\n-1\n\n\n4\n5\n91\nFalse\n0.052494\n1\n\n\n5\n6\n89\nFalse\n0.049740\n1\n\n\n6\n7\n90\nFalse\n0.050532\n1\n\n\n7\n8\n2000\nTrue\n-0.366733\n-1\n\n\n8\n9\n100\nFalse\n0.000000\n1\n\n\n9\n10\n100\nFalse\n0.000000\n1\n\n\n10\n11\n88\nFalse\n0.047625\n1\n\n\n11\n12\n77\nFalse\n0.040150\n1\n\n\n12\n13\n58\nFalse\n0.035680\n1\n\n\n13\n14\n50\nFalse\n0.055380\n1\n\n\n14\n15\n79\nFalse\n0.045024\n1\n\n\n15\n16\n19\nFalse\n0.014319\n1\n\n\n16\n17\n64\nFalse\n0.051140\n1\n\n\n17\n18\n19\nFalse\n0.014319\n1\n\n\n18\n19\n13\nFalse\n0.003310\n1\n\n\n19\n20\n8700\nTrue\n-0.340421\n-1\n\n\n\n\n\n\n\n\n\n8. Calculating the accuracy of the model:\nIt calculates the accuracy of the anomaly detection model.\n\ncorrect_anomalies = student_df[(student_df['anomaly'] == -1) & (student_df['actual_anomaly'])].shape[0]\n\n\ntotal_anomalies = student_df['actual_anomaly'].sum()\n\naccuracy = 100 * correct_anomalies / total_anomalies\naccuracy\n\n100.0\n\n\n\n\n9. Using scatter plots to clustering students with DBSCAN:\n\nfrom sklearn.preprocessing import StandardScaler\nstudent_df['math_score_scaled'] = StandardScaler().fit_transform(student_df[['math_score']])\ndb = DBSCAN(eps=0.2, min_samples=2).fit(student_df[['math_score_scaled']])\n\nstudent_df['dbscan_label'] = db.labels_\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x=student_df.index, y='math_score_scaled', hue='dbscan_label', data=student_df, palette='coolwarm')\nplt.xlabel(\"student_id\")\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "1. Introduction:\nThis blog will utilize both rule-based VADER sentiment analysis tool and machine learning pre-trained RoBERTa model to perform analysis of sentiment in Amazon reviews. It will apply sentiment analysis models to classify reviews into categories such as ‚Äúpositive‚Äù, ‚Äúneutral‚Äù, and ‚Äúnegative‚Äù. The visualizations at end will also show the relationship between different sentiment classifiers.\n\n\n2. Import the libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom tqdm.notebook import tqdm \nfrom scipy.special import softmax\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n\n3. Load the amazon_reviews csv file:\n\namazon_df = pd.read_csv(\"amazon_reviews.csv\")\n\n\n\n4. Bar plot to show the number of reviews in different ratings:\n\nvis = amazon_df['rating'].value_counts().sort_index().plot(kind='bar', title='number of reviews vs. ratings', figsize=(10,5))\nvis.set_xlabel(\"Review ratings\")\nvis.set_ylabel(\"number of reviews\")\nvis.set_xticklabels(vis.get_xticklabels(), rotation=1)\nplt.show()\n\n\n\n\n\n\n5. Apply SentimentIntensityAnalyzer:\nThe code below uses the VADER sentiment analysis tool from NLTK to give scores for each Amazon reviews. Based on the scores, the new column sentiment will classify ‚Äònegative‚Äô, ‚Äòpositive‚Äô, ‚Äòneutral‚Äô based on the scores. The dataframe vader_nltk stores polarity scores and sentiment labels.\n\ndef get_sentiment(score):\n    if score &lt; -0.05:\n        return 'negative'\n    elif score &gt; 0.05:\n        return 'positive'\n    else:\n        return 'neutral'\n\nsia = SentimentIntensityAnalyzer()\nres = {}\nfor i, row in tqdm(amazon_df.iterrows(), total=len(amazon_df)):\n    text = str(row['reviewText'])\n    myid = row['id']\n    polarity_scores = sia.polarity_scores(text)\n    res[myid] = {**polarity_scores, 'sentiment': get_sentiment(polarity_scores['compound'])}\n\nvader_nltk = pd.DataFrame.from_dict(res, orient='index').reset_index()\nvader_nltk.columns = ['id'] + list(vader_nltk.columns[1:])\nvader_nltk = pd.merge(vader_nltk, amazon_df, on='id', how='left')\nvader_nltk.head()\n\n\n\n\n\n\n\n\n\n\n\nid\nneg\nneu\npos\ncompound\nsentiment\nreviewerName\nrating\nreviewText\nreviewTime\nday_diff\nhelpful_yes\nhelpful_no\ntotal_vote\nscore_pos_neg_diff\nscore_average_rating\nwilson_lower_bound\n\n\n\n\n0\n0\n0.688\n0.312\n0.000\n-0.2960\nnegative\nNaN\n4.0\nNo issues.\n2014-07-23\n138\n0\n0\n0\n0\n0.0\n0.0\n\n\n1\n2\n0.077\n0.923\n0.000\n-0.3089\nnegative\n1K3\n4.0\nit works as expected. I should have sprung for...\n2012-12-23\n715\n0\n0\n0\n0\n0.0\n0.0\n\n\n2\n11\n0.000\n1.000\n0.000\n0.0000\nneutral\n98020\n3.0\nIt works, but file writes are a bit slower tha...\n2013-11-20\n383\n0\n0\n0\n0\n0.0\n0.0\n\n\n3\n32\n0.088\n0.774\n0.138\n0.5255\npositive\nAbdulrahman J. Alrashed \"dr34m3r\"\n1.0\nI bought 2 of those SanDisk 32 GB microSD , us...\n2013-03-08\n640\n0\n0\n0\n0\n0.0\n0.0\n\n\n4\n33\n0.000\n0.632\n0.368\n0.9153\npositive\nAbel Feliciano \"Ace Master\"\n4.0\nThe memory card is an excellent condition and ...\n2012-11-09\n759\n0\n0\n0\n0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n6. the boxplot to show the distribution of ‚Äòneg‚Äô, ‚Äòneu‚Äô, ‚Äòpos‚Äô for each rating.\n\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\nsns.boxplot(data=vader_nltk, x='rating', y='neg', ax=axs[0])\nsns.boxplot(data=vader_nltk, x='rating', y='neu', ax=axs[1])\nsns.boxplot(data=vader_nltk, x='rating', y='pos', ax=axs[2])\naxs[0].set_title('Negative sentiment')\naxs[1].set_title('Neutral sentiment')\naxs[2].set_title('Positive sentiment')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n7. Hugging Face Transformers to load a pre-trained model:\nIt sets up a sentiment analysis model based on RoBERTa architecture for analyzing the Amazon reviews.\n\nMODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n\n\n\n8. VADER Sentiment analysis vs.¬†RoBERTa Model Sentiment Analysis:\n\n(1). VADER score about the review at index 30:\n\nexample_review = amazon_df['reviewText'][30]\nexample_review\nprint(\"Vader sentiment analysis scores for the review at index 30 is \", sia.polarity_scores(example_review))\n\nVader sentiment analysis scores for the review at index 30 is  {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6249}\n\n\n\n\n(2).RoBERTa score about the review at index 30:\n\nencoded_review = tokenizer(example_review, return_tensors='pt')\noutput = model(**encoded_review)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\nscores_dict = {\n  'roberta_neg': scores[0],\n  'roberta_neu': scores[1],\n  'roberta_pos': scores[2]\n}\n\nprint(\"Roberta scores for the review at index 30 is \", scores_dict)\n\nRoberta scores for the review at index 30 is  {'roberta_neg': 0.25840575, 'roberta_neu': 0.34866926, 'roberta_pos': 0.39292496}\n\n\n\n\n\n9. RoBERTa score and VADER score Integration:\nThe res dictionary combines and contains both the VADER sentiment analysis result and the pre-trained model RoBERTa model‚Äôs result.\n\ndef roberta_scores(example_review):\n  encoded_review = tokenizer(example_review, return_tensors='pt')\n  output = model(**encoded_review)\n  scores = output[0][0].detach().numpy()\n  scores = softmax(scores)\n  scores_dict = {\n    'roberta_neg': scores[0],\n    'roberta_neu': scores[1],\n    'roberta_pos': scores[2]\n  }\n  return scores_dict\nres = {}\nfor i, row in tqdm(amazon_df.iterrows(), total=len(amazon_df)):\n    try:\n        review = row['reviewText']\n        review_id = row['id']\n        if isinstance(review, str):\n            vader_result = sia.polarity_scores(review)\n            vader_result2 = {f\"vader_{key}\": value for key, value in vader_result.items()}\n            roberta_result = roberta_scores(review)\n            res[review_id] = {**vader_result2, **roberta_result}\n        else:\n            print(f'Skipped id: {review_id} due to non-string review')\n    except RuntimeError:\n        continue\ndict_list = list(res.items())[:5]\nfor item in dict_list:\n    print(item)\n\n\n\n\n(0, {'vader_neg': 0.688, 'vader_neu': 0.312, 'vader_pos': 0.0, 'vader_compound': -0.296, 'roberta_neg': 0.07495943, 'roberta_neu': 0.5892151, 'roberta_pos': 0.33582547})\n(2, {'vader_neg': 0.077, 'vader_neu': 0.923, 'vader_pos': 0.0, 'vader_compound': -0.3089, 'roberta_neg': 0.42375654, 'roberta_neu': 0.4483106, 'roberta_pos': 0.12793283})\n(11, {'vader_neg': 0.0, 'vader_neu': 1.0, 'vader_pos': 0.0, 'vader_compound': 0.0, 'roberta_neg': 0.2745669, 'roberta_neu': 0.4989962, 'roberta_pos': 0.22643691})\n(32, {'vader_neg': 0.088, 'vader_neu': 0.774, 'vader_pos': 0.138, 'vader_compound': 0.5255, 'roberta_neg': 0.54347664, 'roberta_neu': 0.3384522, 'roberta_pos': 0.118071124})\n(33, {'vader_neg': 0.0, 'vader_neu': 0.632, 'vader_pos': 0.368, 'vader_compound': 0.9153, 'roberta_neg': 0.0029975341, 'roberta_neu': 0.026594352, 'roberta_pos': 0.97040814})\n\n\n\n\n10. Merge into dataframe:\n\nmodel_df = pd.DataFrame(res).T\nmodel_df = model_df.reset_index().rename(columns={'index':'id'})\nmodel_df = model_df.merge(amazon_df, how='left')\nmodel_df.head()\n\n\n\n\n\n\n\n\nid\nvader_neg\nvader_neu\nvader_pos\nvader_compound\nroberta_neg\nroberta_neu\nroberta_pos\nreviewerName\nrating\nreviewText\nreviewTime\nday_diff\nhelpful_yes\nhelpful_no\ntotal_vote\nscore_pos_neg_diff\nscore_average_rating\nwilson_lower_bound\n\n\n\n\n0\n0\n0.688\n0.312\n0.000\n-0.2960\n0.074959\n0.589215\n0.335825\nNaN\n4.0\nNo issues.\n2014-07-23\n138\n0\n0\n0\n0\n0.0\n0.0\n\n\n1\n2\n0.077\n0.923\n0.000\n-0.3089\n0.423757\n0.448311\n0.127933\n1K3\n4.0\nit works as expected. I should have sprung for...\n2012-12-23\n715\n0\n0\n0\n0\n0.0\n0.0\n\n\n2\n11\n0.000\n1.000\n0.000\n0.0000\n0.274567\n0.498996\n0.226437\n98020\n3.0\nIt works, but file writes are a bit slower tha...\n2013-11-20\n383\n0\n0\n0\n0\n0.0\n0.0\n\n\n3\n32\n0.088\n0.774\n0.138\n0.5255\n0.543477\n0.338452\n0.118071\nAbdulrahman J. Alrashed \"dr34m3r\"\n1.0\nI bought 2 of those SanDisk 32 GB microSD , us...\n2013-03-08\n640\n0\n0\n0\n0\n0.0\n0.0\n\n\n4\n33\n0.000\n0.632\n0.368\n0.9153\n0.002998\n0.026594\n0.970408\nAbel Feliciano \"Ace Master\"\n4.0\nThe memory card is an excellent condition and ...\n2012-11-09\n759\n0\n0\n0\n0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n11. Heatmap to show the relationship between VADER Sentiment analysis tool and RoBERTa training model:\nThe heatmap is applied here to explore the correlation between the VADER model classifications and the RoBERTa model classifications.\n\nplt.figure(figsize=(4, 3))\ncorrelation = model_df[['vader_neg', 'vader_neu', 'vader_pos', 'roberta_neg', 'roberta_neu', 'roberta_pos']].corr()\nsns.heatmap(correlation, annot=True, cmap='coolwarm')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom the heatmap shown above, the correlation coefficient of two models‚Äô relationship:\n\nvader_neg - roberta_neg: 0.35\nvader_neu - roberta_neu: 0.36\nvader_pos - roberta_pos: 0.5"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "1. Document Clustering Introduction:\nClustering is a fundamental technique in machine learning that involves grouping data with close and similar properties/patterns together. This blog specifically focuses on document clustering and makes an analysis of finding the sentences with closest and highest similarities. There are a couple of steps to achieve this in the later sections: 1. data clean and initialization 2. compute the cosine distance 3. find the closest cluster 4. merge 5. visualization\n\n\n2. Import the libraries:\n\nimport re\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\nfrom scipy.spatial.distance import cosine\n\n\n\n3. Open the document file:\nBelow is the code to open the ‚Äúwords.txt‚Äù file and each line is an element in the sentences list.\n\nwith open('words.txt', 'r') as file:\n    sentences = file.readlines()\nsentences\n\n['In comparison to dogs, cats have not undergone major changes during the domestication process.\\n',\n 'As cat simply catenates streams of bytes, it can be also used to concatenate binary files, where it will just concatenate sequence of bytes.\\n',\n 'A common interactive use of cat for a single file is to output the content of a file to standard output.\\n',\n 'Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals.\\n',\n 'In one, people deliberately tamed cats in a process of artificial selection, as they were useful predators of vermin.\\n',\n 'The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes.\\n',\n 'Domestic cats are similar in size to the other members of the genus Felis, typically weighing between 4 and 5 kg (8.8 and 11.0 lb).\\n',\n 'However, if the output is piped or redirected, cat is unnecessary.\\n',\n 'cat with one named file is safer where human error is a concern - one wrong use of the default redirection symbol \"&gt;\" instead of \"&lt;\" (often adjacent on keyboards) may permanently delete the file you were just needing to read.\\n',\n 'In terms of legibility, a sequence of commands starting with cat and connected by pipes has a clear left-to-right flow of information.\\n',\n 'Cat command is one of the basic commands that you learned when you started in the Unix / Linux world.\\n',\n 'Using cat command, the lines received from stdin can be redirected to a new file using redirection symbols.\\n',\n 'When you type simply cat command without any arguments, it just receives the stdin content and displays it in the stdout.\\n',\n 'Leopard was released on October 26, 2007 as the successor of Tiger (version 10.4), and is available in two editions.\\n',\n 'According to Apple, Leopard contains over 300 changes and enhancements over its predecessor, Mac OS X Tiger.\\n',\n 'As of Mid 2010, some Apple computers have firmware factory installed which will no longer allow installation of Mac OS X Leopard.\\n',\n 'Since Apple moved to using Intel processors in their computers, the OSx86 community has developed and now also allows Mac OS X Tiger and later releases to be installed on non-Apple x86-based computers.\\n',\n \"OS X Mountain Lion was released on July 25, 2012 for purchase and download through Apple's Mac App Store, as part of a switch to releasing OS X versions online and every year.\\n\",\n 'Apple has released a small patch for the three most recent versions of Safari running on OS X Yosemite, Mavericks, and Mountain Lion.\\n',\n 'The Mountain Lion release marks the second time Apple has offered an incremental upgrade, rather than releasing a new cat entirely.\\n',\n \"Mac OS X Mountain Lion installs in place, so you won't need to create a separate disk or run the installation off an external drive.\\n\",\n \"The fifth major update to Mac OS X, Leopard, contains such a mountain of features - more than 300 by Apple's count.\\n\"]\n\n\n\n\n4. Tokenization and Data Clean:\nFor each sentence in the document, it first converts the sentence to lowercase, tokenize the sentence, and removes the empty words. The tokenized_sentences contains a list of cleaned words. Also, the unique_words contains all unique words found across the sentences in the document.\n\nunique_words = set()\ntokenized_sentences = []\n\nfor sentence in sentences:\n    sentence = sentence.lower()\n    words = re.split('[^a-z]', sentence)\n    words = [word for word in words if word]\n    tokenized_sentences.append(words)\n    unique_words.update(words)\nlen(tokenized_sentences)\n\n22\n\n\n\n\n5. Dataframe setup:\nThe dataframe is structured to represent the frequency count of each unique word across sentences. Its columns correspond to the unique words, and each row corresponds to a sentence. The dataframe serves as the representation of the frequency distribution of words in the document.\n\nsentences_df = pd.DataFrame(index=range(len(tokenized_sentences)), columns=list(unique_words))\nsentences_df.fillna(0, inplace=True)\n\nfor idx, sentence in enumerate(tokenized_sentences):\n    for word in sentence:\n        sentences_df.loc[idx, word] += 1\nsentences_df\n\n\n\n\n\n\n\n\nits\nas\nincremental\ntheir\nwon\nuse\nsmall\nchromosomes\narguments\noctober\n...\nmice\nwill\ndogs\nadjacent\nwild\nmajor\nreleases\nwere\nunix\noutput\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n3\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n5\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n8\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n15\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n17\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n19\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n20\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n22 rows √ó 254 columns\n\n\n\n\n\n6. Compute the cosine distance:\nCosine distance is often used as the measure of text similarity between sentences. The code below computes the cosine distance for each sentence in the Dataframe with respect to the sentence at index 2. The cosine_distance is the collection that represents the distance of the sentence at index 2 to the other sentences(include index 2).\n\nnum_rows = sentences_df.shape[0]\ncosine_distance = [cosine(sentences_df.loc[2], sentences_df.loc[i]) for i in range(num_rows)]\ncosine_distance\n\n[0.8644738145642124,\n 0.7908349933664811,\n 0,\n 0.9668503227934102,\n 0.7532824181024156,\n 0.9244071053981545,\n 0.7971629788651561,\n 0.7187156614369028,\n 0.5364474653494467,\n 0.5517892714996024,\n 0.7885277869449276,\n 0.6598319742916955,\n 0.8647753192434373,\n 0.8360159876618425,\n 0.9203180927110404,\n 0.8590185246299518,\n 0.8740118423302576,\n 0.7806366011571673,\n 0.7532824181024156,\n 0.7885277869449276,\n 0.8011019367604613,\n 0.704915554574673]\n\n\n\n\n7. Sort and find the two sentences with the closest distance:\nIn the previous section, we computed the cosine distance and got a list of distances to the sentence at index 2. Now this section is to sort the list and get 2 closest sentences to the sentence at index 2.\n\nindexed_distances = list(enumerate(cosine_distance))\nsorted_distances = sorted(indexed_distances, key=lambda x: x[1])\nclosest_sentences = sorted_distances[1:3]\nclosest_sentences\n\n[(8, 0.5364474653494467), (9, 0.5517892714996024)]\n\n\nAs we can see from the output, the two closest sentences to the sentence at index 2 are the sentence at index 8 and sentence at index 9. In the next section we will use dendrogram to verify and visualize the cosine distances.\n\n\n8. Visualization using dendrogram:\nA dendrogram is a very useful visualization diagram that displays the sequence of merges in hierarchical clustering. By looking at the dendrogram, we can get a sense of how closely related clusters are.\n\nvisual = linkage(sentences_df, method='single', metric=\"cosine\")\nfig = plt.figure(figsize=(5, 5))\ndn = dendrogram(visual)\nplt.show()\n\n\n\n\nAs we can see in the dendrogram, the sentence at index 8 and the sentence at index 9 are exactly the closest distance to the sentence at index 2, which matches with the answers in section 7."
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "1. Introduction:\nThis blog aims to use linear regression model to predict housing prices in CA. It will perform feature selection based on the correlation of housing attributes. The linear regression model is later trained using the selected features. It also includes a scatterplot to assess the model‚Äôs performance and a best-fit line to show accuracy.\n\n\n2. Import libraries:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n3. Open the csv file:\n\nca_df = pd.read_csv('california-housing.csv')\nprint(ca_df.head())\n\n   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                  41          880           129.0   \n1    -122.22     37.86                  21         7099          1106.0   \n2    -122.24     37.85                  52         1467           190.0   \n3    -122.25     37.85                  52         1274           235.0   \n4    -122.25     37.85                  52         1627           280.0   \n\n   population  households  median_income ocean_proximity  house_value  \n0         322         126         8.3252        NEAR BAY       452600  \n1        2401        1138         8.3014        NEAR BAY       358500  \n2         496         177         7.2574        NEAR BAY       352100  \n3         558         219         5.6431        NEAR BAY       341300  \n4         565         259         3.8462        NEAR BAY       342200  \n\n\n\n\n4. Correlation Feature selection:\nThe code calculates the correlation coefficients of the housing attributes. It then selects the attributes that meet the threshold requirement.\n\ncorrelation_with_price = ca_df.drop('ocean_proximity', axis=1).corr()['house_value']\n\nselected_features = correlation_with_price[(correlation_with_price &gt;= 0.1) | (correlation_with_price &lt;= -0.1)].index\n\nprint(f'Selected features: {selected_features}')\n\nSelected features: Index(['latitude', 'housing_median_age', 'total_rooms', 'median_income',\n       'house_value'],\n      dtype='object')\n\n\n\n\n5.Build the linear regression model from the selected correlated features:\nThe linear regression model takes selected features and house_value to train. The model is able to predict house values, and they are evaluated using Mean Squared Error (MSE) and R-squared metrics.\n\nx = ca_df[selected_features].drop('house_value', axis=1)\ny = ca_df['house_value']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=30)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}\\nR-squared: {r2}')\n\nMean Squared Error: 6907727072.880442\nR-squared: 0.4991305863315274\n\n\n\n\n6. Scatterplot to assess the performance of the regression model:\nThe scatterplot is used to visualize the actual vs.¬†predicted housing values from the regression model. The best fit red line is also plotted on the scatter plot.\n\nslope, intercept = np.polyfit(y_test, y_pred, 1)\nbest_fit_line = np.poly1d([slope, intercept])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred)\nplt.plot(y_test, best_fit_line(y_test), 'r-', lw=2)\nplt.xlabel('Actual house value')\nplt.ylabel('Predicted house value')\nplt.title('Actual vs Predicted House Values')\nplt.show()\n\n\n\n\n\n\n7. Predicting the house price with the regression model:\nIn this case, the new house has features with latitude 37.85, housing_median age 50, total_rooms: 2200, median_income: 2.6059\n\n# [latitude, housing_median_age, total_rooms, median_income]\nnew_house = pd.DataFrame([[37.85, 50, 2200, 2.6059]], columns=X_train.columns)\npredicted_price = model.predict(new_house)\nprint(\"Predicted Price: \", predicted_price[0])\n\nPredicted Price:  183161.94654197228"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805",
    "section": "",
    "text": "Anomaly/outlier detection\n\n\n\n\n\n\n\nanomaly detection\n\n\ncode\n\n\nmachine learning\n\n\nDBSCAN\n\n\nIsolation Forest\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nregression\n\n\nscatterplot\n\n\nmachine learning\n\n\nhousing price prediction\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nNLP\n\n\ncode\n\n\nsentiment analysis\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nmachine learning\n\n\npython\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nYouwei Chen\n\n\n\n\n\n\nNo matching items"
  }
]